#!/usr/bin/env python3
"""
æ•°æ®æ‹‰å–è„šæœ¬ (Python ç‰ˆæœ¬)
ä» GitHub API è·å–ç»„ç»‡æˆå‘˜ä¿¡æ¯å¹¶è½¬æ¢ä¸º CSV æ ¼å¼
"""

import os
import sys
import csv
import json
import time
from datetime import datetime, timedelta
from collections import defaultdict
try:
    import requests
except ImportError:
    requests = None
from pathlib import Path

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥å…±äº«æ¨¡å—
sys.path.insert(0, str(Path(__file__).parent.parent))
from bot_filter import is_bot_account

# åŠ è½½ç¯å¢ƒå˜é‡
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    # python-dotenv ä¸æ˜¯å¿…éœ€çš„ï¼Œå¦‚æœæ²¡æœ‰å®‰è£…å°±å¿½ç•¥
    pass

# é…ç½®
CONFIG = {
    'ORG_NAME': os.getenv('GITHUB_ORG', 'datawhalechina'),
    'GITHUB_TOKEN': os.getenv('GITHUB_TOKEN'),
    'OUTPUT_FILE': Path(__file__).parent.parent.parent / 'docs' / 'public' / 'data' / 'members.csv',
    'OUTPUT_JSON_FILE': Path(__file__).parent.parent.parent / 'docs' / 'public' / 'data' / 'members.json',
    # å‘¨commitæ•°æ®æ–‡ä»¶
    'COMMITS_FILE': Path(__file__).parent.parent.parent / 'docs' / 'public' / 'data' / 'commits_weekly.json',
    # å¤´åƒç¼“å­˜ç›®å½•
    'AVATARS_DIR': Path(__file__).parent.parent.parent / 'docs' / 'public' / 'avatars',
    'API_BASE': 'https://api.github.com',
    # æœ€å°è´¡çŒ®è¡Œæ•°é˜ˆå€¼ï¼ˆé™ä½ä»¥åŒ…å«æ›´å¤šè´¡çŒ®è€…ï¼‰
    # ä¿®æ”¹ä¸º 0ï¼Œç¡®ä¿æ‰€æœ‰è´¡çŒ®è€…éƒ½è¢«é‡‡é›†ï¼ŒåŒ…æ‹¬åªæœ‰å°‘é‡ä»£ç å˜æ›´çš„æ–°è´¡çŒ®è€…
    # æ³¨æ„ï¼šGitHub API çš„ contributions å­—æ®µè¡¨ç¤ºä»£ç è¡Œæ•°å˜æ›´ï¼Œä¸æ˜¯ commit æ•°é‡
    'MIN_CONTRIBUTIONS': int(os.getenv('MIN_CONTRIBUTIONS', '0')),
    'MAX_REPOS_PER_PAGE': 100,  # æ¯é¡µæœ€å¤§ä»“åº“æ•°
    'MAX_CONTRIBUTORS_PER_REPO': 100,  # æ¯ä¸ªä»“åº“æœ€å¤§è´¡çŒ®è€…æ•°
    'MAX_USER_REPOS': 100,  # è·å–ç”¨æˆ·ä»“åº“çš„æœ€å¤§æ•°é‡
    'COMMIT_DAYS_RANGE': 7,  # è·å–æœ€è¿‘Nå¤©çš„commitæ•°æ®
    'MAX_COMMITS_PER_REPO': 200,  # æ¯ä¸ªä»“åº“æœ€å¤§commitæ•°
    # topic/å…³é”®è¯ â†’ é¢†åŸŸæ˜ å°„ï¼ˆç”¨äº bioã€topicsã€ä»“åº“ååŒ¹é…ï¼‰
    'DEFAULT_DOMAINS': {
        'machine-learning': 'æœºå™¨å­¦ä¹ ',
        'deep-learning': 'æ·±åº¦å­¦ä¹ ',
        'natural-language-processing': 'NLP',
        'computer-vision': 'è®¡ç®—æœºè§†è§‰',
        'data-mining': 'æ•°æ®æŒ–æ˜',
        'recommendation-system': 'æ¨èç³»ç»Ÿ',
        'reinforcement-learning': 'å¼ºåŒ–å­¦ä¹ ',
        'artificial-intelligence': 'äººå·¥æ™ºèƒ½',
        'llm': 'LLM',
        'data-science': 'æ•°æ®ç§‘å­¦',
        'frontend': 'å‰ç«¯å¼€å‘',
        'backend': 'åç«¯å¼€å‘',
        'fullstack': 'å…¨æ ˆå¼€å‘',
        'bigdata': 'å¤§æ•°æ®',
        'embodied-ai': 'å…·èº«æ™ºèƒ½',
        'robotics': 'å…·èº«æ™ºèƒ½',
        'medical-imaging': 'åŒ»å­¦å½±åƒ',
        'agent': 'AI Agent',
        'multi-agent': 'AI Agent',
        'multimodal': 'å¤šæ¨¡æ€',
        'rag': 'RAG',
        'data-analysis': 'æ•°æ®åˆ†æ',
        'graph-neural-network': 'å›¾ç¥ç»ç½‘ç»œ',
    },
    # ä»“åº“å â†’ é¢†åŸŸ ç›´æ¥æ˜ å°„ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼Œè§£å†³æ­§ä¹‰ï¼‰
    'REPO_DOMAIN_MAP': {
        # é¢è¯•æ±‚èŒï¼ˆä¸æ˜¯ CVï¼‰
        'daily-interview': 'é¢è¯•æ±‚èŒ',
        'get-job': 'é¢è¯•æ±‚èŒ',
        'huawei-od-python': 'é¢è¯•æ±‚èŒ',
        # å…·èº«æ™ºèƒ½ / æœºå™¨äºº
        'easy-robot': 'å…·èº«æ™ºèƒ½',
        'easy-ros2arm': 'å…·èº«æ™ºèƒ½',
        'every-embodied': 'å…·èº«æ™ºèƒ½',
        'ai-hardware-robotics': 'å…·èº«æ™ºèƒ½',
        'white-cloud-robotics': 'å…·èº«æ™ºèƒ½',
        # åŒ»å­¦å½±åƒ
        'med-imaging-primer': 'åŒ»å­¦å½±åƒ',
        # AI Agent
        'agent-tutorial': 'AI Agent',
        'agentic-ai': 'AI Agent',
        'hello-agents': 'AI Agent',
        'wow-agent': 'AI Agent',
        'handy-multi-agent': 'AI Agent',
        'hugging-multi-agent': 'AI Agent',
        'easy-langent': 'AI Agent',
        # AI å·¥å…·æ•™ç¨‹
        'handy-ollama': 'AIå·¥å…·',
        'handy-n8n': 'AIå·¥å…·',
        'self-dify': 'AIå·¥å…·',
        'coze-ai-assistant': 'AIå·¥å…·',
        'self-llm': 'AIå·¥å…·',
        'unlock-hf': 'AIå·¥å…·',
        'unlock-deepseek': 'AIå·¥å…·',
        'smart-prompt': 'AIå·¥å…·',
        'easy-vibe': 'AIå·¥å…·',
        'vibe-vibe': 'AIå·¥å…·',
        # æ¡†æ¶æ•™ç¨‹
        'thorough-pytorch': 'æ¡†æ¶æ•™ç¨‹',
        'openmmlab-tutorial': 'æ¡†æ¶æ•™ç¨‹',
        'd2l-ai-solutions-manual': 'æ¡†æ¶æ•™ç¨‹',
        'fantastic-matplotlib': 'æ¡†æ¶æ•™ç¨‹',
        'joyful-pandas': 'æ¡†æ¶æ•™ç¨‹',
        'powerful-numpy': 'æ¡†æ¶æ•™ç¨‹',
        'wow-plotly': 'æ¡†æ¶æ•™ç¨‹',
        # RAG
        'all-in-rag': 'RAG',
        'wow-rag': 'RAG',
        'easy-vectordb': 'RAG',
        'easy-vecdb': 'RAG',
        # LLM
        'llm-cookbook': 'LLM',
        'llm-universe': 'LLM',
        'llm-deploy': 'LLM',
        'llm-research': 'LLM',
        'llmbook': 'LLM',
        'llms-from-scratch-cn': 'LLM',
        'so-large-lm': 'LLM',
        'happy-llm': 'LLM',
        'base-llm': 'LLM',
        'hugging-llm': 'LLM',
        'hands-on-llm': 'LLM',
        'hands-on-llama': 'LLM',
        'code-your-own-llm': 'LLM',
        'tiny-universe': 'LLM',
        'post-training-of-llms': 'LLM',
        'leegenai-tutorial': 'LLM',
        # å¤šæ¨¡æ€
        'hugging-audio': 'å¤šæ¨¡æ€',
        'sora-tutorial': 'å¤šæ¨¡æ€',
        'hugging-vis': 'å¤šæ¨¡æ€',
        'vced': 'å¤šæ¨¡æ€',
        # è®¡ç®—æœºè§†è§‰ï¼ˆçœŸæ­£çš„ CV é¡¹ç›®ï¼‰
        'dive-into-cv-pytorch': 'è®¡ç®—æœºè§†è§‰',
        'deep-learning-for-computer-vision': 'è®¡ç®—æœºè§†è§‰',
        'magic-cv': 'è®¡ç®—æœºè§†è§‰',
        'team-learning-cv': 'è®¡ç®—æœºè§†è§‰',
        'yolo-master': 'è®¡ç®—æœºè§†è§‰',
        'easy-dip': 'è®¡ç®—æœºè§†è§‰',
        # NLP
        'easy-nlp': 'NLP',
        'base-nlp': 'NLP',
        'hands-dirty-nlp': 'NLP',
        'learn-nlp-with-transformers': 'NLP',
        'team-learning-nlp': 'NLP',
        'hand-bert': 'NLP',
        'fun-transformer': 'NLP',
        # å¼ºåŒ–å­¦ä¹ 
        'easy-rl': 'å¼ºåŒ–å­¦ä¹ ',
        'joyrl': 'å¼ºåŒ–å­¦ä¹ ',
        'joyrl-book': 'å¼ºåŒ–å­¦ä¹ ',
        'hugging-rl': 'å¼ºåŒ–å­¦ä¹ ',
        'key-book': 'å¼ºåŒ–å­¦ä¹ ',
        'fun-marl': 'å¼ºåŒ–å­¦ä¹ ',
        'team-learning-rl': 'å¼ºåŒ–å­¦ä¹ ',
        # æ¨èç³»ç»Ÿ
        'fun-rec': 'æ¨èç³»ç»Ÿ',
        'torch-rechub': 'æ¨èç³»ç»Ÿ',
        'fun-ir': 'æ¨èç³»ç»Ÿ',
        # æ•°æ®ç«èµ›
        'competition-baseline': 'æ•°æ®ç«èµ›',
        'coggle': 'æ•°æ®ç«èµ›',
        # æ•°æ®åˆ†æ
        'hands-on-data-analysis': 'æ•°æ®åˆ†æ',
        'team-learning-data-mining': 'æ•°æ®åˆ†æ',
        # ç¼–ç¨‹åŸºç¡€
        'learn-python-the-smart-way': 'ç¼–ç¨‹åŸºç¡€',
        'learn-python-the-smart-way-v2': 'ç¼–ç¨‹åŸºç¡€',
        'leetcode-notes': 'ç¼–ç¨‹åŸºç¡€',
        'team-learning-program': 'ç¼–ç¨‹åŸºç¡€',
        'team-learning-sql': 'ç¼–ç¨‹åŸºç¡€',
        'wonderful-sql': 'ç¼–ç¨‹åŸºç¡€',
        'cstart': 'ç¼–ç¨‹åŸºç¡€',
        'go-talent': 'ç¼–ç¨‹åŸºç¡€',
        # å…¨æ ˆå¼€å‘
        'wow-fullstack': 'å…¨æ ˆå¼€å‘',
        'whale-web': 'å‰ç«¯å¼€å‘',
        'sweettalk-django': 'åç«¯å¼€å‘',
    }
}


def get_headers():
    """è·å–è¯·æ±‚å¤´"""
    headers = {
        'User-Agent': 'members-visualization-bot',
        'Accept': 'application/vnd.github.v3+json'
    }

    if CONFIG['GITHUB_TOKEN']:
        headers['Authorization'] = f"Bearer {CONFIG['GITHUB_TOKEN']}"

    return headers


def fetch_api(url, retries=3):
    """å‘é€ API è¯·æ±‚ï¼ˆå¸¦é‡è¯•é€»è¾‘ï¼‰"""
    if not CONFIG['GITHUB_TOKEN']:
        print("âš ï¸  æœªè®¾ç½® GITHUB_TOKENï¼Œå¯èƒ½ä¼šé‡åˆ° API é€Ÿç‡é™åˆ¶")

    for attempt in range(retries):
        try:
            print(f"ğŸ”„ è¯·æ±‚ {url} (å°è¯• {attempt + 1}/{retries})")

            response = requests.get(url, headers=get_headers(), timeout=30)

            # æ£€æŸ¥é€Ÿç‡é™åˆ¶
            remaining = response.headers.get('X-RateLimit-Remaining')
            reset_time = response.headers.get('X-RateLimit-Reset')

            if remaining:
                print(f"ğŸ“Š API å‰©ä½™è¯·æ±‚æ¬¡æ•°: {remaining}")

            if response.status_code == 403 and remaining == '0':
                if reset_time and attempt < retries - 1:
                    reset_timestamp = int(reset_time)
                    wait_time = reset_timestamp - int(time.time()) + 1
                    if wait_time > 0:
                        print(f"â³ API é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                        time.sleep(wait_time)
                        continue
                raise requests.exceptions.HTTPError(f"API é€Ÿç‡é™åˆ¶å·²è¾¾ä¸Šé™")

            response.raise_for_status()
            return response.json()

        except requests.RequestException as e:
            print(f"âŒ è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{retries}): {url}")
            print(f"é”™è¯¯: {e}")

            if attempt == retries - 1:
                return None

            # æŒ‡æ•°é€€é¿å»¶è¿Ÿ
            wait_time = (2 ** attempt)
            print(f"â³ ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
            time.sleep(wait_time)

    return None


def get_org_repos(org_name):
    """è·å–ç»„ç»‡ä»“åº“åˆ—è¡¨ï¼ˆæ”¯æŒåˆ†é¡µï¼‰"""
    print(f"æ­£åœ¨è·å–ç»„ç»‡ {org_name} çš„ä»“åº“åˆ—è¡¨...")

    all_repos = []
    page = 1
    per_page = CONFIG['MAX_REPOS_PER_PAGE']

    while True:
        url = f"{CONFIG['API_BASE']}/orgs/{org_name}/repos?per_page={per_page}&page={page}&type=public&sort=updated"
        repos = fetch_api(url)

        if not repos or len(repos) == 0:
            break

        # è¿‡æ»¤æ‰ fork çš„ä»“åº“ï¼Œåªä¿ç•™åŸåˆ›ä»“åº“
        original_repos = [
            repo for repo in repos if not repo.get('fork', False)]
        all_repos.extend(original_repos)

        print(f"è·å–ç¬¬ {page} é¡µï¼š{len(repos)} ä¸ªä»“åº“ï¼ˆ{len(original_repos)} ä¸ªåŸåˆ›ï¼‰")

        # æµ‹è¯•æ¨¡å¼ï¼šé™åˆ¶æ€»ä»“åº“æ•°
        if CONFIG.get('TEST_MODE', False) and len(all_repos) >= CONFIG.get('TEST_MAX_REPOS', 5):
            print(
                f"ğŸ§ª æµ‹è¯•æ¨¡å¼ï¼šå·²è¾¾åˆ°ä»“åº“æ•°é™åˆ¶ ({CONFIG.get('TEST_MAX_REPOS', 5)} ä¸ª)ï¼Œåœæ­¢è·å–")
            all_repos = all_repos[:CONFIG.get('TEST_MAX_REPOS', 5)]  # ç¡®ä¿ä¸è¶…è¿‡é™åˆ¶
            break

        # å¦‚æœè¿”å›çš„ä»“åº“æ•°å°‘äºæ¯é¡µé™åˆ¶ï¼Œè¯´æ˜å·²ç»æ˜¯æœ€åä¸€é¡µ
        if len(repos) < per_page:
            break

        page += 1

        # å®‰å…¨é™åˆ¶ï¼šæœ€å¤šè·å–20é¡µï¼ˆ2000ä¸ªä»“åº“ï¼‰
        if page > 20:
            print("âš ï¸ è¾¾åˆ°é¡µæ•°é™åˆ¶ï¼Œåœæ­¢è·å–")
            break

    print(f"æ€»å…±æ‰¾åˆ° {len(all_repos)} ä¸ªåŸåˆ›ä»“åº“")
    return all_repos


def get_repo_contributors(org_name, repo_name):
    """è·å–ä»“åº“è´¡çŒ®è€…ï¼ˆè¿‡æ»¤æœºå™¨äººè´¦æˆ·ï¼‰"""
    all_contributors = []
    page = 1
    per_page = 100

    while True:
        url = f"{CONFIG['API_BASE']}/repos/{org_name}/{repo_name}/contributors?per_page={per_page}&page={page}"
        contributors = fetch_api(url)

        if not contributors or len(contributors) == 0:
            break

        all_contributors.extend(contributors)

        # å¦‚æœè¿”å›çš„è´¡çŒ®è€…æ•°å°‘äºæ¯é¡µé™åˆ¶ï¼Œè¯´æ˜å·²ç»æ˜¯æœ€åä¸€é¡µ
        if len(contributors) < per_page:
            break

        page += 1

        # å®‰å…¨é™åˆ¶ï¼šæœ€å¤šè·å–10é¡µï¼ˆ1000ä¸ªè´¡çŒ®è€…ï¼‰
        if page > 10:
            print(f"    âš ï¸ ä»“åº“ {repo_name} è´¡çŒ®è€…è¿‡å¤šï¼Œå·²è¾¾é¡µæ•°é™åˆ¶")
            break

    # è¿‡æ»¤æ‰è´¡çŒ®æ•°ä½äºé˜ˆå€¼çš„è´¡çŒ®è€…å’Œæœºå™¨äººè´¦æˆ·
    qualified_contributors = []
    for contributor in all_contributors:
        username = contributor['login']
        contributions = contributor.get('contributions', 0)

        # æ£€æŸ¥æ˜¯å¦ä¸ºæœºå™¨äººè´¦æˆ·
        if is_bot_account(username):
            print(f"    ğŸ¤– è·³è¿‡æœºå™¨äººè´¦æˆ·: {username}")
            continue

        if contributions >= CONFIG['MIN_CONTRIBUTIONS']:
            qualified_contributors.append({
                'login': contributor['login'],
                'contributions': contributions,
                'html_url': contributor['html_url'],
                'avatar_url': contributor['avatar_url']
            })

    print(
        f"    ğŸ“Š æ€»è´¡çŒ®è€…: {len(all_contributors)}, ç¬¦åˆæ¡ä»¶(â‰¥{CONFIG['MIN_CONTRIBUTIONS']}è¡Œ): {len(qualified_contributors)}")
    return qualified_contributors


def collect_contributors_from_repos(org_name):
    """ä»ç»„ç»‡ä»“åº“ä¸­æ”¶é›†è´¡çŒ®è€…æ•°æ®"""
    print(f"ğŸš€ å¼€å§‹ä» {org_name} ç»„ç»‡ä»“åº“æ”¶é›†è´¡çŒ®è€…æ•°æ®...")

    # è·å–ç»„ç»‡æ‰€æœ‰ä»“åº“
    repos = get_org_repos(org_name)
    if not repos:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•ä»“åº“")
        return {}

    # {username: {repos: [repo_names], total_contributions: int, user_info: dict}}
    contributors_data = {}

    for i, repo in enumerate(repos):
        repo_name = repo['name']
        print(f"\nğŸ“ å¤„ç†ä»“åº“ {i + 1}/{len(repos)}: {repo_name}")

        try:
            # è·å–ä»“åº“è´¡çŒ®è€…
            contributors = get_repo_contributors(org_name, repo_name)
            print(
                f"  âœ“ æ‰¾åˆ° {len(contributors)} ä¸ªç¬¦åˆæ¡ä»¶çš„è´¡çŒ®è€…ï¼ˆâ‰¥{CONFIG['MIN_CONTRIBUTIONS']}è¡Œï¼‰")

            for contributor in contributors:
                username = contributor['login']
                contributions = contributor['contributions']

                if username not in contributors_data:
                    contributors_data[username] = {
                        'repos': [],
                        'total_contributions': 0,
                        'user_info': {
                            'html_url': contributor['html_url'],
                            'avatar_url': contributor['avatar_url']
                        }
                    }

                contributors_data[username]['repos'].append(repo_name)
                contributors_data[username]['total_contributions'] += contributions

            # API é€Ÿç‡é™åˆ¶æ§åˆ¶
            delay = 0.1 if CONFIG['GITHUB_TOKEN'] else 0.5
            time.sleep(delay)

        except Exception as e:
            print(f"  âš ï¸ å¤„ç†ä»“åº“ {repo_name} æ—¶å‡ºé”™: {e}")
            continue

    print(f"\nğŸ‰ æ”¶é›†å®Œæˆï¼æ€»å…±å‘ç° {len(contributors_data)} ä¸ªè´¡çŒ®è€…")
    return contributors_data


def download_avatar(avatar_url, username):
    """ä¸‹è½½å¹¶ç¼“å­˜ç”¨æˆ·å¤´åƒ"""
    if not avatar_url or not requests:
        return None

    # ç¡®ä¿å¤´åƒç›®å½•å­˜åœ¨
    CONFIG['AVATARS_DIR'].mkdir(parents=True, exist_ok=True)

    # å¤´åƒæ–‡ä»¶è·¯å¾„
    avatar_filename = f"{username}.jpg"
    avatar_path = CONFIG['AVATARS_DIR'] / avatar_filename

    # å¦‚æœå¤´åƒå·²å­˜åœ¨ï¼Œç›´æ¥è¿”å›ç›¸å¯¹è·¯å¾„
    if avatar_path.exists():
        return f"avatars/{avatar_filename}"

    try:
        print(f"  ğŸ“¸ ä¸‹è½½å¤´åƒ: {username}")
        response = requests.get(avatar_url, timeout=30)
        response.raise_for_status()

        with open(avatar_path, 'wb') as f:
            f.write(response.content)

        return f"avatars/{avatar_filename}"
    except Exception as e:
        print(f"  âš ï¸ å¤´åƒä¸‹è½½å¤±è´¥ {username}: {e}")
        return None


def ensure_avatar_exists(username, avatar_url):
    """ç¡®ä¿æŒ‡å®šç”¨æˆ·çš„å¤´åƒæ–‡ä»¶å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä¸‹è½½"""
    if not username or not avatar_url:
        return False

    # ç¡®ä¿å¤´åƒç›®å½•å­˜åœ¨
    CONFIG['AVATARS_DIR'].mkdir(parents=True, exist_ok=True)

    # å¤´åƒæ–‡ä»¶è·¯å¾„
    avatar_filename = f"{username}.jpg"
    avatar_path = CONFIG['AVATARS_DIR'] / avatar_filename

    # å¦‚æœå¤´åƒå·²å­˜åœ¨ï¼Œæ— éœ€ä¸‹è½½
    if avatar_path.exists():
        return True

    try:
        # é™é»˜ä¸‹è½½å¤´åƒï¼Œé¿å…è¿‡å¤šè¾“å‡º
        response = requests.get(avatar_url, timeout=10)
        response.raise_for_status()

        with open(avatar_path, 'wb') as f:
            f.write(response.content)

        print(f"      ğŸ“¸ æ–°å¢å¤´åƒ: {username}")
        return True

    except Exception as e:
        # é™é»˜å¤„ç†é”™è¯¯ï¼Œé¿å…ä¸­æ–­æ•°æ®æ”¶é›†æµç¨‹
        return False


def get_user_details(username):
    """è·å–ç”¨æˆ·è¯¦ç»†ä¿¡æ¯"""
    url = f"{CONFIG['API_BASE']}/users/{username}"
    return fetch_api(url)


def get_user_repos(username, max_repos=None):
    """è·å–ç”¨æˆ·ä»“åº“ä¿¡æ¯"""
    if max_repos is None:
        max_repos = CONFIG['MAX_USER_REPOS']

    url = f"{CONFIG['API_BASE']}/users/{username}/repos?sort=updated&per_page={max_repos}"
    repos = fetch_api(url)
    return repos if repos else []


def calculate_user_stats(user_details, user_repos):
    """è®¡ç®—ç”¨æˆ·ç»Ÿè®¡ä¿¡æ¯ï¼ˆä¸ªäººä»“åº“æ•°æ®ï¼Œç”¨äºå‚è€ƒï¼‰"""
    if not user_details:
        return {
            'public_repos': 0,
            'total_stars': 0,
            'followers': 0,
            'following': 0
        }

    # ä»ç”¨æˆ·è¯¦æƒ…è·å–åŸºæœ¬ç»Ÿè®¡
    stats = {
        'public_repos': user_details.get('public_repos', 0),
        'followers': user_details.get('followers', 0),
        'following': user_details.get('following', 0),
        'total_stars': 0
    }

    # è®¡ç®—æ€» Starsï¼ˆä»ç”¨æˆ·ä¸ªäººä»“åº“ä¸­ç´¯åŠ ï¼‰
    if user_repos:
        stats['total_stars'] = sum(
            repo.get('stargazers_count', 0) for repo in user_repos)

    return stats


def calculate_org_contribution_stats(username, contrib_info, org_repos_cache):
    """
    è®¡ç®—ç”¨æˆ·åœ¨ç»„ç»‡ä»“åº“ä¸­çš„è´¡çŒ®ç»Ÿè®¡

    Args:
        username: ç”¨æˆ·å
        contrib_info: è´¡çŒ®è€…ä¿¡æ¯ï¼ŒåŒ…å«å‚ä¸çš„ä»“åº“åˆ—è¡¨å’Œè´¡çŒ®æ•°
        org_repos_cache: ç»„ç»‡ä»“åº“ç¼“å­˜ï¼Œæ ¼å¼ä¸º {repo_name: repo_data}

    Returns:
        dict: ç»„ç»‡è´¡çŒ®ç»Ÿè®¡æ•°æ®
    """
    stats = {
        'org_repos_count': 0,        # å‚ä¸çš„ç»„ç»‡ä»“åº“æ•°é‡
        'org_total_stars': 0,         # å‚ä¸çš„ç»„ç»‡ä»“åº“æ€» stars
        'org_total_forks': 0,         # å‚ä¸çš„ç»„ç»‡ä»“åº“æ€» forks
        'org_total_contributions': 0, # åœ¨ç»„ç»‡ä¸­çš„æ€»è´¡çŒ®æ•°ï¼ˆä»£ç è¡Œæ•°ï¼‰
        'org_avg_stars_per_repo': 0   # å¹³å‡æ¯ä¸ªå‚ä¸ä»“åº“çš„ stars
    }

    if not contrib_info or not contrib_info.get('repos'):
        return stats

    participated_repos = contrib_info.get('repos', [])
    stats['org_repos_count'] = len(participated_repos)
    stats['org_total_contributions'] = contrib_info.get('total_contributions', 0)

    # ç´¯åŠ å‚ä¸çš„ç»„ç»‡ä»“åº“çš„ stars å’Œ forks
    for repo_name in participated_repos:
        if repo_name in org_repos_cache:
            repo_data = org_repos_cache[repo_name]
            stats['org_total_stars'] += repo_data.get('stargazers_count', 0)
            stats['org_total_forks'] += repo_data.get('forks_count', 0)

    # è®¡ç®—å¹³å‡å€¼
    if stats['org_repos_count'] > 0:
        stats['org_avg_stars_per_repo'] = stats['org_total_stars'] / stats['org_repos_count']

    return stats


def infer_domains_from_repos(repo_names, user_bio='', user_repos=None):
    """æ ¹æ®ä»“åº“åç§°ç›´æ¥æ˜ å°„ã€topicsã€ç”¨æˆ·ç®€ä»‹æ¨æ–­ç ”ç©¶æ–¹å‘"""
    domains = set()

    # 1. ä¼˜å…ˆï¼šä»“åº“åç›´æ¥æ˜ å°„ï¼ˆæœ€å‡†ç¡®ï¼‰
    repo_domain_map = CONFIG.get('REPO_DOMAIN_MAP', {})
    for repo_name in repo_names:
        if repo_name in repo_domain_map:
            domains.add(repo_domain_map[repo_name])

    # 2. ä»ç”¨æˆ·ç®€ä»‹ä¸­æå–å…³é”®è¯
    text = (user_bio or '').lower()
    for key, value in CONFIG['DEFAULT_DOMAINS'].items():
        if key in text or value.lower() in text:
            domains.add(value)

    # 3. æ”¶é›†æ‰€æœ‰ä»“åº“çš„ topics
    all_topics = []
    if user_repos:
        for repo in user_repos:
            if isinstance(repo, dict) and 'topics' in repo:
                topics = repo.get('topics', [])
                if topics:
                    all_topics.extend(topics)

    # ä» topics ä¸­åŒ¹é…
    topics_text = ' '.join(all_topics).lower()
    for key, value in CONFIG['DEFAULT_DOMAINS'].items():
        if key in topics_text:
            domains.add(value)

    # 4. ä»ä»“åº“åç§°ä¸­åŒ¹é…ï¼ˆè¡¥å……ï¼Œæ’é™¤å·²ç›´æ¥æ˜ å°„çš„ï¼‰
    unmapped_repos = [r for r in repo_names if r not in repo_domain_map]
    repo_text = ' '.join(unmapped_repos).lower()
    for key, value in CONFIG['DEFAULT_DOMAINS'].items():
        if key in repo_text:
            domains.add(value)

    # 5. å…³é”®è¯æ¨¡å¼æ¨æ–­ï¼ˆtopics ä¼˜å…ˆï¼Œä»“åº“åè¡¥å……ï¼‰
    search_text = topics_text if topics_text.strip() else repo_text

    keyword_rules = [
        (['machine-learning', 'sklearn'], 'æœºå™¨å­¦ä¹ '),
        (['deep-learning', 'pytorch', 'tensorflow'], 'æ·±åº¦å­¦ä¹ '),
        (['nlp', 'natural-language', 'bert', 'transformer'], 'NLP'),
        (['recommendation', 'recommender-system', 'ctr-prediction'], 'æ¨èç³»ç»Ÿ'),
        (['computer-vision', 'opencv', 'yolo', 'image-classification'], 'è®¡ç®—æœºè§†è§‰'),
        (['web', 'frontend', 'react', 'vue', 'javascript'], 'å‰ç«¯å¼€å‘'),
        (['gpt', 'llm', 'chatbot', 'llama', 'large-language-model'], 'LLM'),
        (['rag', 'retrieval-augmented'], 'RAG'),
        (['agent', 'multi-agent', 'agentic'], 'AI Agent'),
        (['embodied', 'robotics', 'robot', 'ros2'], 'å…·èº«æ™ºèƒ½'),
        (['medical-imaging', 'medical-image'], 'åŒ»å­¦å½±åƒ'),
        (['multimodal', 'audio', 'speech', 'text-to-image'], 'å¤šæ¨¡æ€'),
        (['reinforcement-learning', 'reinforcement'], 'å¼ºåŒ–å­¦ä¹ '),
        (['hive', 'spark', 'hadoop'], 'å¤§æ•°æ®'),
        (['competition', 'kaggle'], 'æ•°æ®ç«èµ›'),
        (['database', 'sql', 'nosql', 'mongodb', 'mysql'], 'æ•°æ®åº“'),
    ]
    for keywords, domain in keyword_rules:
        if any(kw in search_text for kw in keywords):
            domains.add(domain)

    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•é¢†åŸŸï¼Œè®¾ç½®é»˜è®¤å€¼
    if not domains:
        domains.add('æ•°æ®ç§‘å­¦')

    return list(domains)


def compute_primary_domain(repo_names, domains, repo_commits=None):
    """æ ¹æ®å„é¢†åŸŸçš„commitæ¬¡æ•°ï¼Œè¿”å›commitæœ€å¤šçš„é¢†åŸŸï¼›æ— commitæ•°æ®æ—¶æŒ‰ä»“åº“æ•°å›é€€"""
    repo_domain_map = CONFIG.get('REPO_DOMAIN_MAP', {})
    counts = {}
    if repo_commits:
        for repo, cnt in repo_commits.items():
            d = repo_domain_map.get(repo)
            if d:
                counts[d] = counts.get(d, 0) + cnt
    else:
        for repo in repo_names:
            d = repo_domain_map.get(repo)
            if d:
                counts[d] = counts.get(d, 0) + 1
    if counts:
        return max(counts, key=counts.get)
    return domains[0] if domains else 'æ•°æ®ç§‘å­¦'


def clean_csv_field(text):
    """æ¸…ç†CSVå­—æ®µä¸­çš„æ¢è¡Œç¬¦å’Œå…¶ä»–é—®é¢˜å­—ç¬¦"""
    if not text:
        return ''

    # è½¬æ¢ä¸ºå­—ç¬¦ä¸²å¹¶æ¸…ç†
    text = str(text)

    # æ›¿æ¢æ¢è¡Œç¬¦ä¸ºç©ºæ ¼
    text = text.replace('\n', ' ').replace('\r', ' ')

    # æ›¿æ¢å¤šä¸ªè¿ç»­ç©ºæ ¼ä¸ºå•ä¸ªç©ºæ ¼
    import re
    text = re.sub(r'\s+', ' ', text)

    # å»é™¤é¦–å°¾ç©ºæ ¼
    text = text.strip()

    return text


def save_to_csv(members, output_file):
    """ä¿å­˜æ•°æ®åˆ° CSV æ–‡ä»¶"""
    with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:
        writer = csv.writer(csvfile)

        # å†™å…¥è¡¨å¤´ï¼ˆåŒ…å«æ‰€æœ‰å­—æ®µï¼ŒåŒ…æ‹¬ç»„ç»‡è´¡çŒ®æ•°æ®ï¼‰
        writer.writerow([
            'id', 'name', 'github', 'domain', 'primary_domain', 'repositories',
            'public_repos', 'total_stars', 'followers', 'following',
            'org_repos_count', 'org_total_stars', 'org_total_forks',
            'org_total_contributions', 'org_avg_stars_per_repo',
            'avatar', 'bio', 'location', 'company'
        ])

        # å†™å…¥æ•°æ®
        for member in members:
            writer.writerow([
                clean_csv_field(member['id']),
                clean_csv_field(member['name']),
                clean_csv_field(member['github']),
                ';'.join(member['domains']),
                clean_csv_field(member.get('primary_domain', '')),
                ';'.join(member.get('repositories', [])),
                member.get('public_repos', 0),
                member.get('total_stars', 0),
                member.get('followers', 0),
                member.get('following', 0),
                member.get('org_repos_count', 0),
                member.get('org_total_stars', 0),
                member.get('org_total_forks', 0),
                member.get('org_total_contributions', 0),
                round(member.get('org_avg_stars_per_repo', 0), 2),
                clean_csv_field(member.get('avatar', '')),
                clean_csv_field(member.get('bio', '')),
                clean_csv_field(member.get('location', '')),
                clean_csv_field(member.get('company', ''))
            ])


def save_to_json(members, output_file):
    """ä¿å­˜æ•°æ®åˆ° json æ–‡ä»¶"""
    # å†™å…¥æ•°æ®
    input_data = []

    with open(output_file, 'w', newline='', encoding='utf-8') as jsonfile:
        for member in members:
            input_data.append({
                'id': clean_csv_field(member['id']),
                'name': clean_csv_field(member['name']),
                'github': clean_csv_field(member['github']),
                'domain': ';'.join(member['domains']),
                'primary_domain': member.get('primary_domain', ''),
                'repositories': ';'.join(member.get('repositories', [])),
                # ä¸ªäººæ•°æ®
                'public_repos': member.get('public_repos', 0),
                'total_stars': member.get('total_stars', 0),
                'followers': member.get('followers', 0),
                'following': member.get('following', 0),
                # ç»„ç»‡è´¡çŒ®æ•°æ®
                'org_repos_count': member.get('org_repos_count', 0),
                'org_total_stars': member.get('org_total_stars', 0),
                'org_total_forks': member.get('org_total_forks', 0),
                'org_total_contributions': member.get('org_total_contributions', 0),
                'org_avg_stars_per_repo': round(member.get('org_avg_stars_per_repo', 0), 2),
                # å…¶ä»–ä¿¡æ¯
                'avatar': clean_csv_field(member.get('avatar', '')),
                'bio': clean_csv_field(member.get('bio', '')),
                'location': clean_csv_field(member.get('location', '')),
                'company': clean_csv_field(member.get('company', ''))
            })
        json.dump(input_data, jsonfile, ensure_ascii=False, indent=4)


def check_existing_data():
    """æ£€æŸ¥ç°æœ‰æ•°æ®æ–‡ä»¶"""
    return os.path.exists(CONFIG['OUTPUT_FILE']) or os.path.exists(CONFIG['OUTPUT_JSON_FILE'])


def backup_existing_data():
    """å¤‡ä»½ç°æœ‰æ•°æ®ï¼ˆå·²ç¦ç”¨ï¼Œç›´æ¥è¦†ç›–ï¼‰"""
    # ä¸å†åˆ›å»ºå¤‡ä»½æ–‡ä»¶ï¼Œç›´æ¥è¦†ç›–ä»¥èŠ‚çœç©ºé—´
    if os.path.exists(CONFIG['OUTPUT_FILE']):
        print(f"ğŸ“‹ å‘ç°ç°æœ‰æ•°æ®ï¼Œå°†ç›´æ¥è¦†ç›–: {CONFIG['OUTPUT_FILE']}")
    if os.path.exists(CONFIG['OUTPUT_JSON_FILE']):
        print(f"ğŸ“‹ å‘ç°ç°æœ‰JSONæ•°æ®ï¼Œå°†ç›´æ¥è¦†ç›–: {CONFIG['OUTPUT_JSON_FILE']}")
    return None


def main():
    """ä¸»å‡½æ•° - ç»Ÿä¸€ç‰ˆæœ¬ï¼Œå§‹ç»ˆæ”¶é›†commitæ•°æ®"""
    print("ğŸš€ å¼€å§‹æ‰§è¡Œæ•°æ®æ‹‰å–è„šæœ¬ï¼ˆåŒ…å«commitæ•°æ®ï¼‰...")
    print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {CONFIG['OUTPUT_FILE']}, {CONFIG['OUTPUT_JSON_FILE']}")
    print(f"ğŸ“Š Commitæ•°æ®æ–‡ä»¶: {CONFIG['COMMITS_FILE']}")
    print(f"ğŸ¢ ç»„ç»‡åç§°: {CONFIG['ORG_NAME']}")
    print(f"ğŸ”‘ Token çŠ¶æ€: {'å·²é…ç½®' if CONFIG['GITHUB_TOKEN'] else 'æœªé…ç½®'}")

    # å½“æœªå®‰è£… requests æ—¶ä¼˜é›…é™çº§
    if requests is None:
        print("âš ï¸ ç¼ºå°‘ requests åº“ï¼Œè·³è¿‡ç½‘ç»œè¯·æ±‚ã€‚")
        if check_existing_data():
            print("ğŸ”„ ä½¿ç”¨ç°æœ‰æ•°æ®ç»§ç»­æ„å»º...")
            sys.exit(0)
        else:
            print("ğŸ’¥ æ²¡æœ‰ç°æœ‰æ•°æ®å¯ç”¨ï¼Œæ„å»ºå¤±è´¥")
            sys.exit(1)

    has_existing_data = check_existing_data()
    overall_start_time = time.time()

    try:
        if has_existing_data:
            backup_existing_data()

        # ç»Ÿä¸€æ•°æ®æ”¶é›†ï¼ˆåŒæ—¶è·å–æˆå‘˜å’Œcommitæ•°æ®ï¼‰
        contributors_data, all_commits, org_repos_cache, api_stats = collect_unified_data(
            CONFIG['ORG_NAME'], include_commits=True)

        if not contributors_data:
            print("âš ï¸  æœªæ‰¾åˆ°ä»»ä½•è´¡çŒ®è€…æ•°æ®")
            if has_existing_data:
                print("ğŸ”„ ä½¿ç”¨ç°æœ‰æ•°æ®ç»§ç»­æ„å»º...")
                sys.exit(0)
            else:
                print("ğŸ’¥ æ²¡æœ‰ç°æœ‰æ•°æ®å¯ç”¨ï¼Œæ„å»ºå¤±è´¥")
                sys.exit(1)

        # é¢„å…ˆèšåˆcommitæ•°æ®ï¼Œç”¨äºè®¡ç®—primary_domain
        user_commits_agg = {}
        if all_commits:
            user_commits_agg = aggregate_commits_by_user(all_commits)

        # å¤„ç†æˆå‘˜æ•°æ®
        print(f"\nğŸ‘¥ å¼€å§‹å¤„ç† {len(contributors_data)} ä¸ªæˆå‘˜çš„è¯¦ç»†ä¿¡æ¯...")
        processed_members = []

        for username, contrib_info in contributors_data.items():
            print(f"\nğŸ‘¤ å¤„ç†æˆå‘˜: {username}")

            try:
                # è·å–ç”¨æˆ·è¯¦ç»†ä¿¡æ¯
                user_details = get_user_details(username)
                api_stats['users'] += 1
                api_stats['total'] += 1

                if user_details:
                    print(f"  âœ“ è·å–ç”¨æˆ·ä¿¡æ¯: {user_details.get('name', 'N/A')}")

                # è·å–ç”¨æˆ·ä»“åº“ä¿¡æ¯
                user_repos = get_user_repos(username)
                api_stats['user_repos'] += 1
                api_stats['total'] += 1
                print(f"  âœ“ è·å–ç”¨æˆ·ä»“åº“: {len(user_repos) if user_repos else 0} ä¸ª")

                # è®¡ç®—ç”¨æˆ·ç»Ÿè®¡ä¿¡æ¯ï¼ˆä¸ªäººä»“åº“æ•°æ®ï¼‰
                user_stats = calculate_user_stats(user_details, user_repos)
                print(
                    f"  âœ“ ä¸ªäººç»Ÿè®¡: {user_stats['public_repos']} ä»“åº“, {user_stats['total_stars']} Stars, {user_stats['followers']} å…³æ³¨è€…")

                # è®¡ç®—ç»„ç»‡è´¡çŒ®ç»Ÿè®¡
                org_stats = calculate_org_contribution_stats(
                    username, contrib_info, org_repos_cache)
                print(
                    f"  âœ“ ç»„ç»‡è´¡çŒ®: {org_stats['org_repos_count']} ä¸ªä»“åº“, {org_stats['org_total_stars']} Stars, {org_stats['org_total_contributions']} è´¡çŒ®æ•°")

                # ä¸‹è½½å¹¶ç¼“å­˜å¤´åƒ
                avatar_url = user_details.get(
                    'avatar_url') if user_details else contrib_info['user_info'].get('avatar_url')
                local_avatar = download_avatar(avatar_url, username)

                # æ¨æ–­ç ”ç©¶æ–¹å‘ï¼ˆåŸºäºä»“åº“ topicsã€å‚ä¸çš„ä»“åº“åç§°å’Œç”¨æˆ·ç®€ä»‹ï¼‰
                user_bio = user_details.get('bio') if user_details else ''
                domains = infer_domains_from_repos(
                    contrib_info['repos'], user_bio, user_repos)
                print(f"  âœ“ æ¨æ–­ç ”ç©¶æ–¹å‘: {', '.join(domains)}")

                repo_commits = user_commits_agg.get(username, {}).get('repo_commits', None)
                primary_domain = compute_primary_domain(contrib_info['repos'], domains, repo_commits)

                processed_members.append({
                    'id': username,
                    'name': user_details.get('name') if user_details else username,
                    'github': contrib_info['user_info']['html_url'],
                    'domains': domains,
                    'primary_domain': primary_domain,
                    'repositories': contrib_info['repos'],  # å‚ä¸çš„ç»„ç»‡ä»“åº“åˆ—è¡¨
                    # ä¸ªäººæ•°æ®ï¼ˆä¿ç•™ç”¨äºå‚è€ƒï¼‰
                    'public_repos': user_stats['public_repos'],  # ä¸ªäººå…¬å¼€ä»“åº“æ•°
                    'total_stars': user_stats['total_stars'],  # ä¸ªäººä»“åº“æ€» Stars æ•°
                    'followers': user_stats['followers'],  # å…³æ³¨è€…æ•°
                    'following': user_stats['following'],  # å…³æ³¨æ•°
                    # ç»„ç»‡è´¡çŒ®æ•°æ®ï¼ˆç”¨äºæ¦œå•æ’åï¼‰
                    'org_repos_count': org_stats['org_repos_count'],  # å‚ä¸çš„ç»„ç»‡ä»“åº“æ•°é‡
                    'org_total_stars': org_stats['org_total_stars'],  # å‚ä¸çš„ç»„ç»‡ä»“åº“æ€» stars
                    'org_total_forks': org_stats['org_total_forks'],  # å‚ä¸çš„ç»„ç»‡ä»“åº“æ€» forks
                    'org_total_contributions': org_stats['org_total_contributions'],  # åœ¨ç»„ç»‡ä¸­çš„æ€»è´¡çŒ®æ•°
                    'org_avg_stars_per_repo': org_stats['org_avg_stars_per_repo'],  # å¹³å‡æ¯ä¸ªå‚ä¸ä»“åº“çš„ stars
                    # å…¶ä»–ä¿¡æ¯
                    'avatar': local_avatar,  # æœ¬åœ°å¤´åƒè·¯å¾„
                    'bio': user_details.get('bio') if user_details else '',
                    'location': user_details.get('location') if user_details else '',
                    'company': user_details.get('company') if user_details else ''
                })

            except Exception as e:
                print(f"  âŒ å¤„ç†æˆå‘˜ {username} æ—¶å‡ºé”™: {e}")
                continue

        if processed_members:
            # ä¿å­˜æˆå‘˜æ•°æ®
            save_to_csv(processed_members, CONFIG['OUTPUT_FILE'])
            save_to_json(processed_members, CONFIG['OUTPUT_JSON_FILE'])

            print(f"âœ… æˆåŠŸå¤„ç† {len(processed_members)} ä¸ªæˆå‘˜")

            # å¤„ç†å¹¶ä¿å­˜commitæ•°æ®
            if all_commits:
                print(f"\nğŸ“Š å¤„ç† {len(all_commits)} ä¸ªcommitæ•°æ®...")

                commits_data = {
                    'update_time': datetime.now().isoformat(),
                    'days_range': CONFIG['COMMIT_DAYS_RANGE'],
                    'total_commits': len(all_commits),
                    'total_repos': len(set(commit['repo'] for commit in all_commits)),
                    'user_commits': user_commits_agg,
                    'optimization_stats': {
                        'api_calls': api_stats,
                        'execution_time': f"{time.time() - overall_start_time:.1f}s",
                        'optimization_enabled': True
                    }
                }

                save_commits_data(commits_data)

            # æ˜¾ç¤ºæ‰§è¡Œç»Ÿè®¡
            total_time = time.time() - overall_start_time
            print(f"\nğŸ‰ æ‰§è¡Œå®Œæˆ!")
            print(f"ğŸ“Š æ€§èƒ½ç»Ÿè®¡:")
            print(f"  - æ€»APIè°ƒç”¨: {api_stats['total']} æ¬¡")
            print(f"  - æ€»æ‰§è¡Œæ—¶é—´: {total_time:.1f} ç§’")

        else:
            print("âŒ æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•æˆå‘˜")
            if has_existing_data:
                print("ğŸ”„ ä½¿ç”¨ç°æœ‰æ•°æ®ç»§ç»­æ„å»º...")
                sys.exit(0)
            else:
                print("ğŸ’¥ æ„å»ºå¤±è´¥")
                sys.exit(1)

    except Exception as e:
        print(f"ğŸ’¥ è„šæœ¬æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

        if has_existing_data:
            print("ğŸ”„ ä½¿ç”¨ç°æœ‰æ•°æ®ç»§ç»­æ„å»º...")
            sys.exit(0)
        else:
            print("ğŸ’¥ æ²¡æœ‰ç°æœ‰æ•°æ®å¯ç”¨ï¼Œæ„å»ºå¤±è´¥")
            sys.exit(1)


def get_recent_commits_for_repo(org_name, repo_name, days=7):
    """è·å–æŒ‡å®šä»“åº“æœ€è¿‘Nå¤©çš„commitæ•°æ®"""

    # è®¡ç®—æ—¶é—´èŒƒå›´
    since_date = datetime.now() - timedelta(days=days)
    since_iso = since_date.isoformat() + 'Z'

    url = f"{CONFIG['API_BASE']}/repos/{org_name}/{repo_name}/commits"
    params = {
        'since': since_iso,
        'per_page': CONFIG['MAX_COMMITS_PER_REPO']
    }

    try:
        response = requests.get(
            url, headers=get_headers(), params=params, timeout=30)
        if response.status_code == 200:
            commits = response.json()
            print(f"  ğŸ“Š ä»“åº“ {repo_name}: è·å–åˆ° {len(commits)} ä¸ªcommit")
            return commits
        else:
            print(
                f"  âš ï¸  ä»“åº“ {repo_name}: è·å–commitå¤±è´¥ (çŠ¶æ€ç : {response.status_code})")
            return []
    except Exception as e:
        print(f"  âŒ ä»“åº“ {repo_name}: è·å–commitå¼‚å¸¸: {e}")
        return []


def process_commits_data(commits, repo_name):
    """å¤„ç†commitæ•°æ®ï¼Œæå–å…³é”®ä¿¡æ¯"""

    processed_commits = []

    for commit in commits:
        try:
            # æå–commitä¿¡æ¯
            commit_data = {
                'sha': commit['sha'][:8],  # çŸ­SHA
                # ç¬¬ä¸€è¡Œæ¶ˆæ¯ï¼Œé™åˆ¶é•¿åº¦
                'message': commit['commit']['message'].split('\n')[0][:100],
                'author': {
                    'name': commit['commit']['author']['name'],
                    'email': commit['commit']['author']['email'],
                    'date': commit['commit']['author']['date']
                },
                'repo': repo_name,
                'url': commit['html_url']
            }

            # å°è¯•è·å–GitHubç”¨æˆ·å
            if commit.get('author') and commit['author']:
                commit_data['github_username'] = commit['author']['login']
            else:
                # å¦‚æœæ²¡æœ‰GitHubç”¨æˆ·ä¿¡æ¯ï¼Œå°è¯•ä»emailæ¨æ–­
                commit_data['github_username'] = None

            # è§£ææ—¥æœŸ
            commit_date = datetime.fromisoformat(
                commit_data['author']['date'].replace('Z', '+00:00'))
            commit_data['date_parsed'] = commit_date
            commit_data['date_str'] = commit_date.strftime('%Y-%m-%d')
            commit_data['hour'] = commit_date.hour

            processed_commits.append(commit_data)

        except Exception as e:
            print(f"    âš ï¸  å¤„ç†commitæ•°æ®æ—¶å‡ºé”™: {e}")
            continue

    return processed_commits


def collect_weekly_commits_data(org_name, days=7):
    """æ”¶é›†ç»„ç»‡æ‰€æœ‰ä»“åº“çš„å‘¨commitæ•°æ®"""
    print(f"ğŸš€ å¼€å§‹æ”¶é›† {org_name} ç»„ç»‡æœ€è¿‘ {days} å¤©çš„commitæ•°æ®...")

    # è·å–ç»„ç»‡ä»“åº“åˆ—è¡¨
    repos = get_org_repos(org_name)
    if not repos:
        print("âŒ æ— æ³•è·å–ç»„ç»‡ä»“åº“åˆ—è¡¨")
        return {}

    all_commits = []
    processed_repos = 0

    for repo in repos:
        repo_name = repo['name']
        print(f"ğŸ“ å¤„ç†ä»“åº“: {repo_name} ({processed_repos + 1}/{len(repos)})")

        # è·å–ä»“åº“çš„commitæ•°æ®
        commits = get_recent_commits_for_repo(org_name, repo_name, days)

        if commits:
            # å¤„ç†commitæ•°æ®
            processed_commits = process_commits_data(commits, repo_name)
            all_commits.extend(processed_commits)

        processed_repos += 1

        # æ·»åŠ å»¶è¿Ÿé¿å…APIé€Ÿç‡é™åˆ¶
        time.sleep(0.5)

        # æ¯å¤„ç†10ä¸ªä»“åº“æ˜¾ç¤ºè¿›åº¦
        if processed_repos % 10 == 0:
            print(f"  âœ… å·²å¤„ç† {processed_repos}/{len(repos)} ä¸ªä»“åº“")

    print(f"ğŸ“Š æ€»å…±æ”¶é›†åˆ° {len(all_commits)} ä¸ªcommit")

    # æŒ‰ç”¨æˆ·èšåˆcommitæ•°æ®
    user_commits = aggregate_commits_by_user(all_commits)

    return {
        'update_time': datetime.now().isoformat(),
        'days_range': days,
        'total_commits': len(all_commits),
        'total_repos': len(repos),
        'user_commits': user_commits,
        'raw_commits': all_commits[:1000]  # åªä¿å­˜å‰1000ä¸ªåŸå§‹commitç”¨äºè°ƒè¯•
    }


def aggregate_commits_by_user(commits):
    """æŒ‰ç”¨æˆ·èšåˆcommitæ•°æ®"""
    from collections import defaultdict

    user_stats = defaultdict(lambda: {
        'total_commits': 0,
        'repos': set(),
        'daily_commits': defaultdict(int),
        'hourly_distribution': defaultdict(int),
        'commit_messages': [],
        'first_commit_date': None,
        'last_commit_date': None
    })

    for commit in commits:
        # ç¡®å®šç”¨æˆ·æ ‡è¯†ï¼ˆä¼˜å…ˆä½¿ç”¨GitHubç”¨æˆ·åï¼Œå¦åˆ™ä½¿ç”¨é‚®ç®±ï¼‰
        user_key = commit.get('github_username') or commit['author']['email']

        if not user_key:
            continue

        stats = user_stats[user_key]

        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        stats['total_commits'] += 1
        stats['repos'].add(commit['repo'])
        stats['daily_commits'][commit['date_str']] += 1
        stats['hourly_distribution'][commit['hour']] += 1

        # ä¿å­˜commitæ¶ˆæ¯ï¼ˆæœ€å¤šä¿å­˜10ä¸ªï¼‰
        if len(stats['commit_messages']) < 10:
            stats['commit_messages'].append({
                'message': commit['message'],
                'repo': commit['repo'],
                'date': commit['date_str'],
                'url': commit['url']
            })

        # æ›´æ–°æ—¶é—´èŒƒå›´
        commit_date = commit['date_parsed']
        if not stats['first_commit_date'] or commit_date < stats['first_commit_date']:
            stats['first_commit_date'] = commit_date
        if not stats['last_commit_date'] or commit_date > stats['last_commit_date']:
            stats['last_commit_date'] = commit_date

    # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
    result = {}
    for user_key, stats in user_stats.items():
        result[user_key] = {
            'total_commits': stats['total_commits'],
            'repos': list(stats['repos']),
            'repo_count': len(stats['repos']),
            'daily_commits': dict(stats['daily_commits']),
            'hourly_distribution': dict(stats['hourly_distribution']),
            'commit_messages': stats['commit_messages'],
            'first_commit_date': stats['first_commit_date'].isoformat() if stats['first_commit_date'] else None,
            'last_commit_date': stats['last_commit_date'].isoformat() if stats['last_commit_date'] else None,
            'active_days': len(stats['daily_commits']),
            'avg_commits_per_day': stats['total_commits'] / max(len(stats['daily_commits']), 1)
        }

    return result


def save_commits_data(commits_data):
    """ä¿å­˜commitæ•°æ®åˆ°JSONæ–‡ä»¶"""
    try:
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        CONFIG['COMMITS_FILE'].parent.mkdir(parents=True, exist_ok=True)

        with open(CONFIG['COMMITS_FILE'], 'w', encoding='utf-8') as f:
            json.dump(commits_data, f, ensure_ascii=False, indent=2)

        print(f"ğŸ’¾ commitæ•°æ®å·²ä¿å­˜åˆ°: {CONFIG['COMMITS_FILE']}")
        return True

    except Exception as e:
        print(f"âŒ ä¿å­˜commitæ•°æ®å¤±è´¥: {e}")
        return False


def collect_unified_data(org_name, include_commits=False):
    """
    ä¼˜åŒ–çš„ç»Ÿä¸€æ•°æ®æ”¶é›†å‡½æ•°
    åœ¨å•æ¬¡éå†ä¸­åŒæ—¶æ”¶é›†æˆå‘˜ä¿¡æ¯å’Œcommitæ•°æ®
    """
    print(f"ğŸš€ å¼€å§‹ç»Ÿä¸€æ•°æ®æ”¶é›† (åŒ…å«commit: {include_commits})...")

    # æ€§èƒ½ç›‘æ§å˜é‡
    api_calls = {
        'repos_list': 0,
        'contributors': 0,
        'commits': 0,
        'users': 0,
        'user_repos': 0,
        'total': 0
    }
    start_time = time.time()

    # è·å–ç»„ç»‡ä»“åº“åˆ—è¡¨ï¼ˆåªè°ƒç”¨ä¸€æ¬¡ï¼‰
    print("ğŸ“ è·å–ç»„ç»‡ä»“åº“åˆ—è¡¨...")
    repos = get_org_repos(org_name)
    api_calls['repos_list'] = 1
    api_calls['total'] += 1

    if not repos:
        print("âŒ æ— æ³•è·å–ç»„ç»‡ä»“åº“åˆ—è¡¨")
        return None, None, api_calls

    print(f"âœ… æ‰¾åˆ° {len(repos)} ä¸ªä»“åº“")

    # åˆå§‹åŒ–æ•°æ®ç»“æ„
    contributors_data = {}  # è´¡çŒ®è€…ä¿¡æ¯
    all_commits = []       # æ‰€æœ‰commitè®°å½•
    org_repos_cache = {}   # ç»„ç»‡ä»“åº“ç¼“å­˜ {repo_name: repo_data}
    processed_repos = 0

    # è®¡ç®—æ—¶é—´èŒƒå›´ï¼ˆç”¨äºcommitè¿‡æ»¤ï¼‰
    if include_commits:
        since_date = datetime.now() - \
            timedelta(days=CONFIG['COMMIT_DAYS_RANGE'])
        since_iso = since_date.isoformat() + 'Z'

    # å•æ¬¡éå†æ‰€æœ‰ä»“åº“ï¼ŒåŒæ—¶æ”¶é›†è´¡çŒ®è€…å’Œcommitæ•°æ®
    for repo in repos:
        repo_name = repo['name']
        print(f"\nğŸ“¦ å¤„ç†ä»“åº“: {repo_name} ({processed_repos + 1}/{len(repos)})")

        try:
            # ç¼“å­˜ä»“åº“æ•°æ®ï¼ˆç”¨äºåç»­è®¡ç®—ç»„ç»‡è´¡çŒ®ç»Ÿè®¡ï¼‰
            org_repos_cache[repo_name] = {
                'name': repo_name,
                'stargazers_count': repo.get('stargazers_count', 0),
                'forks_count': repo.get('forks_count', 0),
                'watchers_count': repo.get('watchers_count', 0),
                'open_issues_count': repo.get('open_issues_count', 0)
            }

            # 1. è·å–ä»“åº“è´¡çŒ®è€…ä¿¡æ¯
            print(f"  ğŸ‘¥ è·å–è´¡çŒ®è€…...")
            contributors_url = f"{CONFIG['API_BASE']}/repos/{org_name}/{repo_name}/contributors"
            contributors_params = {
                'per_page': CONFIG['MAX_CONTRIBUTORS_PER_REPO']}

            contributors_full_url = f"{contributors_url}?per_page={contributors_params['per_page']}"
            contributors = fetch_api(contributors_full_url)
            api_calls['contributors'] += 1
            api_calls['total'] += 1

            if contributors:
                print(f"    âœ“ æ‰¾åˆ° {len(contributors)} ä¸ªè´¡çŒ®è€…")

                # å¤„ç†è´¡çŒ®è€…æ•°æ®
                for contributor in contributors:
                    if contributor['contributions'] >= CONFIG['MIN_CONTRIBUTIONS']:
                        username = contributor['login']

                        # æ£€æŸ¥æ˜¯å¦ä¸ºæœºå™¨äººè´¦æˆ·
                        if is_bot_account(username):
                            print(f"    ğŸ¤– è·³è¿‡æœºå™¨äººè´¦æˆ·: {username}")
                            continue

                        if username not in contributors_data:
                            contributors_data[username] = {
                                'user_info': contributor,
                                'repos': [],
                                'total_contributions': 0
                            }

                        contributors_data[username]['repos'].append(repo_name)
                        contributors_data[username]['total_contributions'] += contributor['contributions']

            # 2. è·å–commitæ•°æ®ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if include_commits:
                print(f"  ğŸ“Š è·å–commitæ•°æ®...")
                commits_url = f"{CONFIG['API_BASE']}/repos/{org_name}/{repo_name}/commits"
                commits_params = {
                    'since': since_iso,
                    'per_page': CONFIG['MAX_COMMITS_PER_REPO']
                }

                commits_full_url = f"{commits_url}?since={commits_params['since']}&per_page={commits_params['per_page']}"
                commits = fetch_api(commits_full_url)
                api_calls['commits'] += 1
                api_calls['total'] += 1

                if commits:
                    print(f"    âœ“ æ‰¾åˆ° {len(commits)} ä¸ªcommit")

                    # å¤„ç†commitæ•°æ®
                    for commit in commits:
                        try:
                            commit_data = {
                                'sha': commit['sha'][:8],
                                'message': commit['commit']['message'].split('\n')[0][:100],
                                'author': {
                                    'name': commit['commit']['author']['name'],
                                    'email': commit['commit']['author']['email'],
                                    'date': commit['commit']['author']['date']
                                },
                                'repo': repo_name,
                                'url': commit['html_url']
                            }

                            # å°è¯•è·å–GitHubç”¨æˆ·å
                            if commit.get('author') and commit['author']:
                                commit_data['github_username'] = commit['author']['login']
                                # è·å–å¤´åƒURLç”¨äºåç»­ä¸‹è½½
                                commit_data['author_avatar_url'] = commit['author'].get(
                                    'avatar_url')
                            else:
                                commit_data['github_username'] = None
                                commit_data['author_avatar_url'] = None

                            # æ£€æŸ¥æ˜¯å¦ä¸ºæœºå™¨äººè´¦æˆ·çš„æäº¤
                            if commit_data['github_username'] and is_bot_account(commit_data['github_username']):
                                print(
                                    f"      ğŸ¤– è·³è¿‡æœºå™¨äººæäº¤: {commit_data['github_username']}")
                                continue

                            # æ£€æŸ¥å¹¶ä¸‹è½½æ–°å‘ç°è´¡çŒ®è€…çš„å¤´åƒ
                            if commit_data['github_username'] and commit_data['author_avatar_url']:
                                ensure_avatar_exists(
                                    commit_data['github_username'], commit_data['author_avatar_url'])

                                # å¦‚æœè¿™ä¸ªç”¨æˆ·ä¸åœ¨ contributors_data ä¸­ï¼Œæ·»åŠ è¿›å»
                                # è¿™æ ·å¯ä»¥ç¡®ä¿æ‰€æœ‰æœ‰ commit çš„ç”¨æˆ·éƒ½ä¼šè¢«é‡‡é›†åˆ° members.json
                                if commit_data['github_username'] not in contributors_data:
                                    contributors_data[commit_data['github_username']] = {
                                        'user_info': {
                                            'login': commit_data['github_username'],
                                            'html_url': f"https://github.com/{commit_data['github_username']}",
                                            'avatar_url': commit_data['author_avatar_url']
                                        },
                                        'repos': [repo_name],
                                        'total_contributions': 1  # è‡³å°‘æœ‰ 1 ä¸ª commit
                                    }
                                    print(f"      â• æ–°å¢è´¡çŒ®è€…ï¼ˆæ¥è‡ªcommitï¼‰: {commit_data['github_username']}")
                                elif repo_name not in contributors_data[commit_data['github_username']]['repos']:
                                    # å¦‚æœç”¨æˆ·å·²å­˜åœ¨ä½†è¿™ä¸ªä»“åº“ä¸åœ¨åˆ—è¡¨ä¸­ï¼Œæ·»åŠ ä»“åº“
                                    contributors_data[commit_data['github_username']]['repos'].append(repo_name)

                            # è§£ææ—¥æœŸ
                            commit_date = datetime.fromisoformat(
                                commit_data['author']['date'].replace('Z', '+00:00'))
                            commit_data['date_parsed'] = commit_date
                            commit_data['date_str'] = commit_date.strftime(
                                '%Y-%m-%d')
                            commit_data['hour'] = commit_date.hour

                            # è½¬æ¢ä¸ºåŒ—äº¬æ—¶é—´ï¼ˆUTC+8ï¼‰
                            beijing_time = commit_date + timedelta(hours=8)
                            commit_data['beijing_hour'] = beijing_time.hour
                            commit_data['beijing_time'] = beijing_time.isoformat()

                            # åˆ¤æ–­æ˜¯å¦ä¸ºæ·±å¤œæ—¶æ®µï¼ˆåŒ—äº¬æ—¶é—´22:00-06:00ï¼‰
                            is_night_owl = beijing_time.hour >= 22 or beijing_time.hour < 6
                            commit_data['is_night_owl'] = is_night_owl

                            all_commits.append(commit_data)

                        except Exception as e:
                            print(f"      âš ï¸  å¤„ç†commitæ•°æ®æ—¶å‡ºé”™: {e}")
                            continue

            processed_repos += 1

            # æ¯å¤„ç†10ä¸ªä»“åº“æ˜¾ç¤ºè¿›åº¦
            if processed_repos % 10 == 0:
                elapsed = time.time() - start_time
                print(
                    f"  ğŸ“ˆ è¿›åº¦: {processed_repos}/{len(repos)} ä»“åº“ | è€—æ—¶: {elapsed:.1f}s | APIè°ƒç”¨: {api_calls['total']}")

        except Exception as e:
            print(f"  âŒ å¤„ç†ä»“åº“ {repo_name} æ—¶å‡ºé”™: {e}")
            continue

    # ç»Ÿè®¡ç»“æœ
    elapsed_time = time.time() - start_time
    print(f"\nğŸ“Š æ•°æ®æ”¶é›†å®Œæˆ:")
    print(f"  - å¤„ç†ä»“åº“: {processed_repos}/{len(repos)}")
    print(f"  - ç¼“å­˜ç»„ç»‡ä»“åº“: {len(org_repos_cache)} ä¸ª")
    print(f"  - å‘ç°è´¡çŒ®è€…: {len(contributors_data)} äºº")
    if include_commits:
        print(f"  - æ”¶é›†commit: {len(all_commits)} ä¸ª")
    print(f"  - APIè°ƒç”¨ç»Ÿè®¡: {api_calls}")
    print(f"  - æ€»è€—æ—¶: {elapsed_time:.1f} ç§’")

    return contributors_data, all_commits if include_commits else None, org_repos_cache, api_calls


def aggregate_commits_by_user(all_commits):
    """èšåˆcommitæ•°æ®æŒ‰ç”¨æˆ·åˆ†ç»„"""

    user_stats = defaultdict(lambda: {
        'total_commits': 0,
        'repos': set(),
        'repo_commits': defaultdict(int),
        'daily_commits': defaultdict(int),
        'hourly_distribution': defaultdict(int),
        'beijing_hourly_distribution': defaultdict(int),
        'night_owl_commits': 0,
        'commit_messages': [],
        'first_commit_date': None,
        'last_commit_date': None
    })

    for commit in all_commits:
        # å°è¯•è·å–GitHubç”¨æˆ·å
        username = commit.get('github_username')
        if not username:
            # å¦‚æœæ²¡æœ‰GitHubç”¨æˆ·åï¼Œå°è¯•ä»emailæ¨æ–­
            email = commit['author']['email']
            if email and '@' in email:
                username = email.split('@')[0]
            else:
                continue  # è·³è¿‡æ— æ³•è¯†åˆ«ç”¨æˆ·çš„commit

        # åŒé‡æ£€æŸ¥ï¼šç¡®ä¿ä¸æ˜¯æœºå™¨äººè´¦æˆ·
        if is_bot_account(username):
            continue  # è·³è¿‡æœºå™¨äººè´¦æˆ·çš„commit

        stats = user_stats[username]

        # æ›´æ–°ç»Ÿè®¡
        stats['total_commits'] += 1
        stats['repos'].add(commit['repo'])
        stats['repo_commits'][commit['repo']] += 1
        stats['daily_commits'][commit['date_str']] += 1
        stats['hourly_distribution'][commit['hour']] += 1
        stats['beijing_hourly_distribution'][commit['beijing_hour']] += 1

        # ç»Ÿè®¡æ·±å¤œæäº¤
        if commit.get('is_night_owl', False):
            stats['night_owl_commits'] += 1

        # ä¿å­˜commitæ¶ˆæ¯ï¼ˆæœ€å¤š10ä¸ªï¼‰
        if len(stats['commit_messages']) < 10:
            stats['commit_messages'].append({
                'message': commit['message'],
                'repo': commit['repo'],
                'date': commit['date_str'],
                'time': commit.get('beijing_time', ''),
                'beijing_hour': commit.get('beijing_hour', 0),
                'is_night_owl': commit.get('is_night_owl', False),
                'url': commit['url']
            })

        # æ›´æ–°æ—¶é—´èŒƒå›´
        commit_date = commit['date_parsed']
        if not stats['first_commit_date'] or commit_date < stats['first_commit_date']:
            stats['first_commit_date'] = commit_date
        if not stats['last_commit_date'] or commit_date > stats['last_commit_date']:
            stats['last_commit_date'] = commit_date

    # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–æ ¼å¼
    result = {}
    for username, stats in user_stats.items():
        if stats['total_commits'] >= 1:  # è‡³å°‘1ä¸ªcommit
            result[username] = {
                'total_commits': stats['total_commits'],
                'repos': list(stats['repos']),
                'repo_commits': dict(stats['repo_commits']),
                'repo_count': len(stats['repos']),
                'daily_commits': dict(stats['daily_commits']),
                'hourly_distribution': dict(stats['hourly_distribution']),
                'beijing_hourly_distribution': dict(stats['beijing_hourly_distribution']),
                'night_owl_commits': stats['night_owl_commits'],
                'night_owl_percentage': round((stats['night_owl_commits'] / stats['total_commits']) * 100, 1) if stats['total_commits'] > 0 else 0,
                'commit_messages': stats['commit_messages'],
                'first_commit_date': stats['first_commit_date'].isoformat() if stats['first_commit_date'] else None,
                'last_commit_date': stats['last_commit_date'].isoformat() if stats['last_commit_date'] else None,
                'active_days': len(stats['daily_commits']),
                'avg_commits_per_day': stats['total_commits'] / max(len(stats['daily_commits']), 1)
            }

    return result


def save_commits_data(commits_data):
    """ä¿å­˜commitæ•°æ®åˆ°æ–‡ä»¶"""
    try:
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        CONFIG['COMMITS_FILE'].parent.mkdir(parents=True, exist_ok=True)

        # ç›´æ¥ä¿å­˜åˆ°å‰ç«¯ç›®å½•
        with open(CONFIG['COMMITS_FILE'], 'w', encoding='utf-8') as f:
            json.dump(commits_data, f, ensure_ascii=False, indent=2)

        print(f"ğŸ’¾ Commitæ•°æ®å·²ä¿å­˜:")
        print(f"  - æ–‡ä»¶è·¯å¾„: {CONFIG['COMMITS_FILE']}")
        print(
            f"  - æ´»è·ƒç”¨æˆ·: {commits_data.get('user_commits', {}) and len(commits_data['user_commits'])} äºº")
        print(f"  - æ€»commitæ•°: {commits_data.get('total_commits', 0)}")

        return True

    except Exception as e:
        print(f"âŒ ä¿å­˜commitæ•°æ®å¤±è´¥: {e}")
        return False


def test():
    """æµ‹è¯•å‡½æ•° - ä½¿ç”¨è¾ƒå°çš„é…ç½®å€¼è¿›è¡Œå¿«é€Ÿæœ¬åœ°æµ‹è¯•"""
    print("ğŸ§ª å¼€å§‹æµ‹è¯•æ¨¡å¼...")

    # ä¸´æ—¶è¦†ç›–é…ç½®å€¼ä»¥åŠ å¿«æµ‹è¯•é€Ÿåº¦ï¼ˆåªé™åˆ¶æ€»ä»“åº“æ•°å’Œæ€»è´¡çŒ®è€…æ•°ï¼‰
    original_config = {}
    test_config = {
        'MAX_REPOS_PER_PAGE': 100,   # ä¿æŒæ­£å¸¸çš„æ¯é¡µä»“åº“æ•°
        'MAX_CONTRIBUTORS_PER_REPO': 10,  # é™åˆ¶æ¯ä¸ªä»“åº“çš„è´¡çŒ®è€…æ•°ï¼ˆæ§åˆ¶æ€»è´¡çŒ®è€…æ•°ï¼‰
    }

    # è®¾ç½®æµ‹è¯•æ¨¡å¼æ ‡å¿—ï¼Œç”¨äºé™åˆ¶æ€»ä»“åº“æ•°
    CONFIG['TEST_MODE'] = True
    CONFIG['TEST_MAX_REPOS'] = 15  # æµ‹è¯•æ¨¡å¼ä¸‹æœ€å¤šå¤„ç†5ä¸ªä»“åº“

    # ä¿å­˜åŸå§‹é…ç½®å¹¶åº”ç”¨æµ‹è¯•é…ç½®
    for key, value in test_config.items():
        original_config[key] = CONFIG[key]
        CONFIG[key] = value
        print(f"  ğŸ“ {key}: {original_config[key]} â†’ {value}")

    print(f"  â„¹ï¸  ä¿æŒåŸæœ‰é…ç½®:")
    print(f"     MIN_CONTRIBUTIONS = {CONFIG['MIN_CONTRIBUTIONS']} (è´¡çŒ®é˜ˆå€¼ä¸å˜)")
    print(f"     COMMIT_DAYS_RANGE = {CONFIG['COMMIT_DAYS_RANGE']} å¤©")
    print(
        f"  ğŸ¯ æµ‹è¯•é¢„æœŸ: æœ€å¤šå¤„ç† {test_config['MAX_REPOS_PER_PAGE']} ä¸ªä»“åº“ï¼Œæ¯ä¸ªä»“åº“æœ€å¤š {test_config['MAX_CONTRIBUTORS_PER_REPO']} ä¸ªè´¡çŒ®è€…")

    try:
        # è¿è¡Œä¸»å‡½æ•°ï¼ˆç°åœ¨é»˜è®¤åŒ…å«commitæ•°æ®æ”¶é›†ï¼‰
        main()
    finally:
        # æ¢å¤åŸå§‹é…ç½®
        for key, value in original_config.items():
            CONFIG[key] = value
        # æ¸…ç†æµ‹è¯•æ¨¡å¼æ ‡å¿—
        CONFIG.pop('TEST_MODE', None)
        CONFIG.pop('TEST_MAX_REPOS', None)
        print("ğŸ”„ å·²æ¢å¤åŸå§‹é…ç½®")


if __name__ == '__main__':
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1:
        if sys.argv[1] == '--test':
            test()
        else:
            print("âŒ æœªçŸ¥å‚æ•°ã€‚æ”¯æŒçš„å‚æ•°ï¼š--test")
            print("ğŸ’¡ æç¤ºï¼šè„šæœ¬ç°åœ¨é»˜è®¤æ”¶é›†commitæ•°æ®ï¼Œæ— éœ€ --with-commits å‚æ•°")
            sys.exit(1)
    else:
        main()
